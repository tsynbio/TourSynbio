# TourSynbioLM<sup>TM</sup>
<div align="center">


[![OpenXLab_Model][OpenXLab_Model-image]][OpenXLab_Model-url] 

[OpenXLab_Model-image]: https://cdn-static.openxlab.org.cn/header/openxlab_models.svg
[OpenXLab_App-image]: https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg

[OpenXLab_Model-url]: https://openxlab.org.cn/models/detail/ZanTourSynbio/TourSynbio-7B
</div>

## ç›®å½• <!-- omit in toc -->
- [ç®€ä»‹](#ç®€ä»‹)
- [News](#news)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
  - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
    - [ä¸‹è½½æ¨¡å‹](#ä¸‹è½½æ¨¡å‹)
    - [æœ¬åœ°éƒ¨ç½²](#æœ¬åœ°éƒ¨ç½²)
  - [XTunerå¾®è°ƒæŒ‡å—](#xtunerå¾®è°ƒæŒ‡å—)
- [è‡´è°¢](#è‡´è°¢)
  - [é¡¹ç›®æˆå‘˜](#é¡¹ç›®æˆå‘˜)
- [å¼€æºè®¸å¯è¯](#å¼€æºè®¸å¯è¯)

## ç®€ä»‹
[TourSynbio<sup>TM</sup>]()è›‹ç™½è´¨å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªèåˆäº†è›‹ç™½è´¨é¢†åŸŸçŸ¥è¯†çš„å…ˆè¿›æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºInternLM2-Chat-7Bï¼Œä½¿ç”¨Xtunerå·¥å…·åŒ…è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶é‡‡ç”¨äº†[ProteinLMBench](https://huggingface.co/datasets/tsynbio/ProteinLMBench)ä¸­çš„SFTï¼ˆSupervised Fine-Tuningï¼‰æ•°æ®é›†ã€‚TourSynbio<sup>TM</sup>ä¸ä»…å…·å¤‡ç†è§£äººç±»è¯­è¨€çš„èƒ½åŠ›ï¼Œè¿˜èƒ½å¤Ÿç†è§£è›‹ç™½è´¨åºåˆ—â€”â€”è¿™ä¸€ç”Ÿå‘½çš„è¯­è¨€ã€‚å®ƒå®ç°äº†è›‹ç™½è´¨ä¸“ä¸šé¢†åŸŸä¸æ™®é€šè¯­è¨€çš„æ— ç¼å¯¹æ¥ï¼Œä½¿å¤æ‚æ•°æ®å’Œä¿¡æ¯å˜å¾—æ›´æ˜“ç†è§£å’Œåº”ç”¨ã€‚é€šè¿‡æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»å¤æ‚æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯å’Œè§è§£ï¼ŒåŠ é€Ÿç§‘å­¦å‘ç°çš„è¿›ç¨‹ã€‚

## News

[2024.06.23] TourSynbio<sup>TM</sup>(SFT only)å·²å¼€æºä¸Šçº¿ã€‚


## ä½¿ç”¨æ–¹æ³•
### å¿«é€Ÿå¼€å§‹
#### ä¸‹è½½æ¨¡å‹
<details>

<summary>ä»OpenXLab</summary>

å‚è€ƒ [ä¸‹è½½æ¨¡å‹](https://openxlab.org.cn/docs/models/%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B.html) ã€‚

```bash
pip install openxlab
```

```python
from openxlab.model import download
download(model_repo=[model_link]', 
        model_name='[model_link]', output='./')
```

</details>



#### æœ¬åœ°éƒ¨ç½²
1. ä»Githubè·å–é¡¹ç›®ä»£ç 
    ```bash
    git clone (ourlink)
    python (å¼€å§‹æ‰§è¡Œæ–‡ä»¶å)
    ```

2. åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
    ```bash
    conda env create -f enviroment.yml
    conda activate (envName)
    pip install -r requirements.txt
    ```
3. Demoè¿è¡Œ
    ```bash
    streamlit run web_demo.py --server.address=0.0.0.0 --server.prot=8501
    ```
### XTunerå¾®è°ƒæŒ‡å—

*   å‰è¨€

XTuner æ”¯æŒå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ã€‚æ•°æ®é›†é¢„å¤„ç†æŒ‡å—è¯·æŸ¥é˜…[æ–‡æ¡£](https://github.com/InternLM/xtuner/blob/main/docs/zh_cn/user_guides/single_turn_conversation.md), å¾®è°ƒæ–¹æ³•æŒ‡å—è¯·æŸ¥é˜…[æ–‡æ¡£](https://github.com/InternLM/xtuner/blob/main/docs/zh_cn/user_guides/finetune.md)ã€‚

*   æ­¥éª¤1ï¼Œå°†æ•°æ®æ„é€ æˆXTuner å®šä¹‰çš„[å•è½®å¯¹è¯æ•°æ®æ ¼å¼](https://github.com/InternLM/xtuner/blob/main/docs/zh_cn/user_guides/dataset_format.md#å•è½®å¯¹è¯æ•°æ®é›†æ ¼å¼), å½¢å¦‚

        [{
            "conversation":[
                {
                    "system": "xxx",
                    "input": "xxx",
                    "output": "xxx"
                }
            ]
        },
        {
            "conversation":[
                {
                    "system": "xxx",
                    "input": "xxx",
                    "output": "xxx"
                }
            ]
        }]

        # demo
        {
                "conversation": [
                    {
                        "system": "Please evaluate the following protein sequence and provide an explanation of the enzyme's catalytic activity, including the chemical reaction it facilitates: ",
                        "intput": "<seq> M P G R Q L T E L L T G L E E V K V Q T A M E Q K E M M I G G L T A D S R E V R P G D L F A A L P G A R V D G R D F I D Q A V G R G A D V V L A P V G T S L K D Y G R P V S L V T S D E P R R T L A Q M A A R F H G R Q P R T I A A V T G T S G K T S V A D F L R Q I W T L A D R K A A S L G T L G L I P A T A A S K A P P Y L T T P D P V A L H A C L K E V A E A G Y E H L A L E A S S H G L D Q Y R L D G L T F S A A A F T N L S Q D H L D Y H P D M E S Y L N A K A R L F G D L L P T G A T A V L N A D A P E F D R L A A L C E R R G I E V L S Y G L A G D D L R I V E A R A L P D G I A L S L R V K G Q D W Q G K L D L I G T F Q G H N V L A A L G L A L A T G L E P S V A L E A L P K L V G V P G R L Q R V A Q T V S G A Q V F V D Y A H K P G A L E A A L T A L R P H A E G R L I V V F G A G G D R D R G K R P L M G E I A T R L A D V V L V T D D N P R S E D P V A I R A E I L A A A P G A R E V S D R G G A I A A A L A E A D P G D L V L I A G K G H E T G Q I V G D K V L P F D D S E I A R R L A R G G Q V </seq>",
                        "output": "By examining the input protein sequence, the enzyme catalyzes the subsequent chemical reaction: ATP + meso-2,6-diaminoheptanedioate + UDP-N-acetyl-alpha-D-   muramoyl-L-alanyl-D-glutamate = ADP + H(+) + phosphate + UDP-N-   acetyl-alpha-D-muramoyl-L-alanyl-gamma-D-glutamyl-meso-2,6-   diaminoheptanedioate."
                    }
                ]
        }

*   **æ­¥éª¤ 2**ï¼Œå‡†å¤‡é…ç½®æ–‡ä»¶ã€‚XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

    ```shell
    xtuner list-cfg
    ```

    æˆ–è€…ï¼Œå¦‚æœæ‰€æä¾›çš„é…ç½®æ–‡ä»¶ä¸èƒ½æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œè¯·å¯¼å‡ºæ‰€æä¾›çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œç›¸åº”æ›´æ”¹ï¼š

    ```shell
    xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
    vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
    ```

â€‹        é…ç½®configæ–‡ä»¶ï¼Œå¯ä»¥å…ˆcopyå®˜æ–¹çš„`internlm2-chat-7b`çš„configæ–‡ä»¶ï¼Œç„¶åå°†copyçš„configæ–‡ä»¶ï¼Œé‡å‘½åä¸º` internlm2_7b_protein_lora.py`ç„¶åè¿›è¡Œä¿®æ”¹,

    ...
    custom_hooks = [
        dict(
            tokenizer=dict(
                padding_side='right',
                pretrained_model_name_or_path=
                '/cpfs01/shared/gmai/xtuner_workspace/internlm/internlm2-7b/', # PATH/TO/PRETRAINED MODELS
                trust_remote_code=True,
                type='transformers.AutoTokenizer.from_pretrained'),
            type='xtuner.engine.DatasetInfoHook'),
    ]
    data_path = [
        '/cpfs01/shared/gmai/xtuner_workspace/protein_data/formated_ssl_data/sll_data_0.json', # PATH/TO/DATA
    	...
    ]
    ...
    model = dict(
        llm=dict(
            pretrained_model_name_or_path=
            '/cpfs01/shared/gmai/xtuner_workspace/internlm/internlm2-7b/', # PATH/TO/PRETRAINED MODELS
            torch_dtype='torch.float16',
            trust_remote_code=True,
            type='transformers.AutoModelForCausalLM.from_pretrained'),
        lora=dict(    # LoRA
            bias='none',
            lora_alpha=16,
            lora_dropout=0.1,
            r=64,
            task_type='CAUSAL_LM',
            type='peft.LoraConfig'),
        type='xtuner.model.SupervisedFinetune')
     ...

ä¸»è¦ä¿®æ”¹çš„åœ°æ–¹å°±æ˜¯ï¼Œ**é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„**ï¼Œ**æ•°æ®è·¯å¾„**ï¼Œä»¥åŠ**å¾®è°ƒæ–¹å¼ï¼ˆLoRAï¼‰**ï¼Œå…¶ä»–çš„è¶…å‚ï¼ŒæŒ‰éœ€è°ƒæ•´å³å¯ï¼Œè¿™é‡Œæˆ‘ä»¬ä¿æŒé»˜è®¤ã€‚

**æ³¨æ„**ï¼š

â€‹	SFTé˜¶æ®µä¸SSLé˜¶æ®µéƒ½æ˜¯é€šè¿‡ä¿®æ”¹configæ–‡ä»¶è¿›è¡Œçš„ï¼Œä¿®æ”¹æ–¹æ³•å¹¶æ— å·®åˆ«ã€‚ä½†æ˜¯SSLçš„æ•°æ®åœ¨æ•°æ®æ„é€ æ—¶`input`ä¸ºç©ºï¼Œè¯¦ç»†çš„pretrainedæ•°æ®æ„é€ è§[æ–‡æ¡£](https://github.com/InternLM/xtuner/blob/main/docs/zh_cn/user_guides/incremental_pretraining.md)ã€‚

*   **æ­¥éª¤ 3**ï¼Œå¼€å§‹å¾®è°ƒã€‚

    ```shell
    xtuner train internlm2_7b_protein_lora
    ```

    ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ LoRA ç®—æ³•åœ¨ è›‹ç™½è´¨ (XXX) æ•°æ®é›†ä¸Šå¾®è°ƒ InternLM2-Chat-7Bï¼š

    ```shell
    # å•å¡
    xtuner train internlm2_7b_protein_lora --deepspeed deepspeed_zero2
    # å¤šå¡
    (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_7b_protein_lora --deepspeed deepspeed_zero2
    (SLURM) srun ${SRUN_ARGS} xtuner train internlm2_7b_protein_lora --launcher slurm --deepspeed deepspeed_zero2
    ```

    *   `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

    *   æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥é˜…[æ–‡æ¡£](./docs/zh_cn/user_guides/finetune.md)ã€‚

*   **æ­¥éª¤ 4**ï¼Œå°†ä¿å­˜çš„ PTH æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼š

    ```shell
    xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
    ```

## è‡´è°¢
### é¡¹ç›®æˆå‘˜

 -  é™ˆèµ (é€”æ·±æ™ºåˆ)
 -  æ²ˆé€¸å¿ ï¼ˆé€”æ·±æ™ºåˆï¼‰
 -  ææ·»æ–Œ ï¼ˆä¸Šæµ·AI Labï¼‰
 -  ä½•å†›å†› ï¼ˆä¸Šæµ·AI Labï¼‰
 -  ç‹å®‡å…‰ ï¼ˆé€”æ·±æ™ºåˆ ä¸Šæµ·äº¤é€šå¤§å­¦ï¼‰

## å¼€æºè®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨[Apache License 2.0 å¼€æºè®¸å¯è¯](https://github.com/tsynbio/TourSynbio/blob/main/LICENSE)ã€‚
